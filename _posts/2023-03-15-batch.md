---
title:  "배치(Batch)"
excerpt: "머신러닝에서 사용되는 배치와 관련된 개념 정리"

categories:
  - ML
tags:
  - [ML, Batch]

toc: true
toc_sticky: true

published: true

date: 2023-03-15
last_modified_at: 2023-09-19
---

## 배치(batch) 관련 내용 정리  
헷갈리는 배치와 미니배치에 대한 개념과 학습할 때 보통 배치 사이즈를 정하는데 왜 사용하는지, 어떻게 진행되는건지에 대한 정리를 해보려고 한다.  
<br>  

### 배치(Batch)  
**전체 데이터셋을 일정한 크기의 묶음(batch)으로 나누는 것**  

주어직 작업에 대한 데이터를 일괄 처리하기 위해 사용하며, 데이터를 몇 개씩 묶어서 학습할 것인지가 배치 사이즈가 된다.  

데이터를 1개씩 입력받아 매번 연산하는 것보다 한번에 n개의 데이터를 묶음으로 입력 받아 연속으로 연산을 진행하면 중간중간에 I/O를 통해 데이터를 읽어오면서 느려지는 횟수를 줄여 효율적이고 빠르게 학습시킬 수 있다.  

단순한 모델에 적은 데이터를 학습시키는 경우 그냥 학습해도 큰 문제가 되지 않지만, 신경망의 깊이가 깊어지고 노드 수가 많아짐에 따라 연산량이 급격히 증가하게 되면서 한번 학습을 완료하는데 너무 많은 시간이 소요될 수 있다.   모델을 설계하고 성능을 테스트 해보는 과정에서 몇번의 학습이 수행될지 모르는데 가능하면 빠르게 끝내는 것이 좋을 것이다.  
<br><br>  

### 미니배치(Mini-Batch)  
**데이터셋의 묶음 하나**  

각 미니배치에 포함된 데이터의 개수가 미니배치 사이즈로 배치 사이즈에 의해 결정된다. (배치 사이즈와 동일)  
따라서 전체 데이터 수를 미니배치 사이즈로 나눈 수가 미치배치의 개수이다.  
일반적으로 미니배치 사이즈를 32, 64, 128 등 2의 제곱수로 설정하면 GPU에서의 연산이 최적화된다. 학습의 안정성이나 수렴 속도를 고려해 적절하게 조절한다.  
<br><br>  

### 미니배치 학습  
전체 데이터셋을 작은 배치로 나누어 각 미니배치를 사용해 학습하는 방식  

데이터를 하나씩 입력시킬 경우, 신경망을 한번 학습시키는데 걸리는 시간이 짧지만 전체 학습에 대한 시간은 길어지고 각각의 그래디언트가 상당히 불안정해 데이터의 작은 변동에도 학습에 큰 영향을 미쳐 노이즈에 민감해진다. 또한 최적의 가중치 값을 찾는 과정에서 국소적인 최솟값(local minimum)에 빠져 전역 최솟값(global minimum)으로 가지 못하고 헤매는 일이 자주 발생할 수 있다.  

반면에 전체 데이터셋을 한번에 입력시킬 경우, 학습을 완료하는데 걸리는 시간을 단축할 수 있지만 신경망을 한번 학습시키는데 걸리는 시간이 매우 길어진다.  

따라서 전체 데이터셋을 한번에 처리하는 '배치 학습(Batch Learning)'과 전체 데이터를 하나의 데이터 포인트로 간주해 가중치를 업데이트하는 '확률적 경사하강법(SGD)'을 결합한 미니 배치 학습을 이용하면 전체 데이터를 모두 반영하면서 장점을 가져올 수 있다.  
<br>  

#### ↪ 미니배치를 사용하면 학습에 좋은 이유  
배치 사이즈를 n으로 설정해 미치배치 하나에 n개의 데이터가 들어있는 경우 
전체 데이터셋에 대한 메모리가 아닌 미니배치 크기만큼의 메모리만 요구되기 때문에 메모리 요구량이 줄어든다.  
또한, 학습 알고리즘의 성능과 학습 시간을 조절할 수 있다.  
<br>  

#### ↪ 미니배치 학습 과정  
1. 전체 데이터셋을 배치 사이즈의 데이터를 가진 미니배치들로 나눈다.  
2. 각 미니배치를 순차적으로 가져온다.  
3. 해당 미니배치에 있는 데이터로 손실함수를 계산한다.  
4. 손실함수의 그래디언트(도함수)를 계산한다.  
5. 계산된 그래디언트를 사용하여 가중치를 업데이트한다.  

한 미니배치에 대한 처리가 완료될 때마다 가중치를 업데이트하며, 이전 미니배치에서 가중치 결과는 다음 미니배치의 처리에 영향을 미친다.  
결과적으로 학습 중에 가중치가 지속적으로 조정이 되며, 전체 데이터에 적합하게 학습이 완료된다.  
<br>  

#### ↪ 병렬로 미니배치 학습을 하면 한번에 나오는 그래디언트들을 어떻게 할까?  
미니배치 내에서의 연산은 GPU에서 병렬로 이루어지지만 가중치 업데이트는 미니배치 하나가 처리될 때마다 이루어진다. 동시에 여러개의 미니배치가 처리되었다고 하더라도 가중치 업데이트는 순차적으로 진행된다.  
<br>  

#### ↪ CPU에서 미니배치 학습을 해도 상관없을까?  
미니배치 학습은 하드웨어 리소스와 독립적이기 때문에 CPU에서도 미니배치 학습은 가능하지만 시간이 오래걸리고 GPU를 사용할 경우 여러 코어를 사용해 병렬 처리가 가능하므로 대규모 데이터나 복잡한 연산에 이점이 있다.  
<br>  

#### ↪ 60000개 데이터셋에 배치 사이즈가 100일 경우  
- 전체 데이터셋 크기 : 60000
- 배치 사이즈(각 미니배치의 크기) : 100
- 미니배치 개수 : 600
<br><br>  

### 에폭(Epoch)  

<br><br>  

### SGD

<br><br>  

### 배치 정규화(Batch Nomalization)  
- 문제점  
  - Internal Coveriate Shift  
  네트워크의 각 레이어를 통과할 때마다 입력 분포가 변하는 현상  
  입력 분포가 변화함에 따라 학습이 어려워지고 수렴 속도가 늦어지게 되는 문제가 발생할 수 있다.  
  - Vanishing Gradients  
  신경망의 각 레이어를 통과할 때마다 그래디언트가 사라지는 현상  

주로 합성곱 신경망(CNN)이나 완전 연결 신경망(Fully Connected Neural Networks)에서 사용되어 성능을 향상 시킨다.  
