---
title:  "병렬(Parallel) 처리"
excerpt: "병렬 처리 방법 공부"

categories:
  - ML
tags:
  - [ML, Parallel]

toc: true
toc_sticky: true

published: true

date: 2023-03-20
last_modified_at: 2023-10-13
---

##   
- [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255){:target="_blank"}  
- [Multi-GPU 학습 과정](https://medium.com/daangn/pytorch-multi-gpu-%ED%95%99%EC%8A%B5-%EC%A0%9C%EB%8C%80%EB%A1%9C-%ED%95%98%EA%B8%B0-27270617936b){:target="_blank"}  

https://tutorials.pytorch.kr/beginner/dist_overview.html  

### Data Parallel Training  
- Pytorch  
  - DataParallel  
    - Single-Machine, Multi-GPU  
  - DistributedDataParallel  
    - Single-Machine, Multi-GPU  
    - Multi-Machine, Multi-GPU  

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc = nn.Linear(784, 10)

    def forward(self, x):
        return self.fc(x)

device = torch.device("cuda:0")

model = Net()
model.to(device)
model = nn.DataParallel(model)
```

- criterion  
모델의 예측값과 실제 타겟 간의 차이를 측정하는 '손실 함수(lost function)'  
ex) 평균 제곱 오차(Mean Squared Error, MSE), 교차 엔트로피 오차(Cross-Entropy Error), ...  

- scatter  
Pytorch에서 tensor의 특정 위치에 값을 할당하는 함수  
딥러닝에서 iteration 마다 batch를 사용하는 GPU의 개수만큼 나누는 작업  

1. 매 iteration마다 Batch를 GPU의 개수만큼 나눈다(scatter).
2. 모델을 각 GPU에 복사하여 할당한다(replicate).
3. 각 GPU에서 foward를 진행한다.
4. 각 GPU에서 input에 대한 출력값이 나오면 이들을 하나의 GPU에 모은다(gather).
5. 하나의 GPU에 모인 값들을 이용하여 loss gradients를 계산한다.
6. 각 GPU로 gradients를 scatter하고 각 GPU에 있는 모델은 계산된 gradient를 가지게 된다.
7. 각 GPU에서 Back-Propagation을 진행한다.
8. 각 GPU에 있는 gradient를 다시 하나의 GPU로 모아서 모델을 업데이트한다. 




## Pytorch 모델 클래스 정의하기  
- nn.Sequential()로 정의  
  ```python
  import torch
  import torch.nn as nn
  from torchvision import models

  num_classes = 5

  model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
  num_ftrs = model.fc.in_features
  model.fc = nn.Sequential(
      nn.Linear(num_ftrs, 512),
      nn.BatchNorm1d(512),
      nn.ReLU(inplace=True),
      nn.Dropout(),
      nn.Linear(512, 256),
      nn.BatchNorm1d(256),
      nn.ReLU(inplace=True),
      nn.Dropout(),
      nn.Linear(256, num_classes)
  )
  ```

- 클래스 사용  
  ```python
  import torch
  import torch.nn as nn
  import torchvision.models as models

  resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
  # backbone = nn.Sequential(*list(resnet50.children())[:-1])  # 출력층 사용X

  class CustomResNet(nn.Module):
      def __init__(self, resnet50, num_classes):
          super(CustomResNet, self).__init__()
          self.resnet50 = resnet50
          num_ftrs = self.resnet50.fc.in_features

          self.linear0 = nn.Linear(num_ftrs, 512)
          self.bn0 = nn.BatchNorm1d(512)
          self.relu0 = nn.ReLU(inplace=True)
          self.dropout0 = nn.Dropout()
          self.linear1 = nn.Linear(512, 256)
          self.bn1 = nn.BatchNorm1d(256)
          self.relu1 = nn.ReLU(inplace=True)
          self.dropout1 = nn.Dropout()
          self.linear2 = nn.Linear(256, num_classes)

      def forward(self, x):
          x = self.resnet50(x)
          x = self.linear0(x)
          x = self.bn0(x)
          x = self.relu0(x)
          x = self.dropout0(x)
          x = self.linear1(x)
          x = self.bn1(x)
          x = self.relu1(x)
          x = self.dropout1(x)
          x = self.linear2(x)
          return x

  num_classes = 5
  custom_resnet = CustomResNet(resnet50, num_classes)
  ```
